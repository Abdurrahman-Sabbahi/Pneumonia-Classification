# -*- coding: utf-8 -*-
"""Pneumonia(Sabbahi).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Egbcv_ghJWvdhFQxEMoGVYsRtmn6gJeE
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from google_drive_downloader import GoogleDriveDownloader as gdd

gdd.download_file_from_google_drive(file_id='1d_93d9oFNRBK9Vg6BRxs9wvRbKtNTylY',
                                    dest_path='content/pneumonia_dataset.zip',
                                    unzip=True)

import pandas as pd                                     # Data analysis and manipultion tool
import numpy as np                                      # Fundamental package for linear algebra and multidimensional arrays
import tensorflow as tf                                 # Deep Learning Tool
import os                                               # OS module in Python provides a way of using operating system dependent functionality
import cv2                                              # Library for image processing
from sklearn.model_selection import train_test_split    # For splitting the data into train and validation set
from sklearn.metrics import accuracy_score
from keras.layers.normalization import BatchNormalization
import matplotlib.pyplot as plt

data=[]                                                                                 # we created an empty array called data
img_size = 100                                                                          # defined img_size to be 100
def create_data():                                                                      # defined a func to create the data
        for item in ['normal','pneumonia']:                                             # for loop that iterates for both items of the list separately!
            path='/content/content/pneumonia_dataset/train/' + item+"/"
            
            for img in os.listdir(path):                                                # os.listdir gets you all the list of name of files located in the given path, " for each image do the following"
                try:
                    img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)   # converts the image to pixels and gray scales the images
                    new_img_array=cv2.resize(img_array,(img_size,img_size))
                    # print(img_array)
                    if item == 'normal':
                        data.append([new_img_array,0])
                    else:
                        data.append([new_img_array, 1])                                 # appending the list of image pixels and respective target value in data
                except Exception as e:
                    pass                                                                # try and except is exception handling case in python, saves you from getting errors
                
            
create_data()

len(data) #Total number of records including both normal and pneumonia cases

data[1400] # This is a case with pneumonia as can be seen through the label associated

plt.imshow(data[1400][0],cmap='gray') #pneumonia

from google.colab.patches import cv2_imshow

cv2_imshow(cv2.resize(data[1400][0], (0,0), fx=2.5, fy=2.5)) #pneumonia

data[1] #normal

plt.imshow(data[1][0],cmap='gray')#normal

cv2_imshow(cv2.resize(data[1][0], (0,0), fx=2.5, fy=2.5)) #normal

np.random.shuffle(data)

X = []
y = []
for image in data:
  X.append(image[0])
  y.append(image[1])

type(X)

# converting x & y to numpy array as they are list
X = np.array(X)
y = np.array(y)

type(X)

X[1]

np.unique(y, return_counts=True)

X =  X.reshape(-1, 100, 100, 1)

X.shape

# split the data
X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2, random_state = 42)

X_train.shape

X_train[1]

cv2_imshow(cv2.resize(X_train[1], (0,0), fx=2.5, fy=2.5))

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout,SpatialDropout2D
from keras.optimizers import Adam
from functools import partial
from tensorflow import keras

RegularizedDense=partial(keras.layers.Dense,
                         activation='elu',
                         kernel_initializer='he_normal',
                         kernel_regularizer=keras.regularizers.l2(l2=0.01))

def create_model():
  cnn_model=Sequential([
                        Conv2D(filters=64,kernel_size=3,activation='relu',input_shape=(100,100,1)),
                        Conv2D(filters=64,kernel_size=3,activation='relu',input_shape=(100,100,1)),
                        BatchNormalization(),
                        MaxPooling2D(pool_size=3),
                        Conv2D(filters=128,kernel_size=2,activation='relu',input_shape=(100,100,1)),
                        Conv2D(filters=128,kernel_size=2,activation='relu',input_shape=(100,100,1)),
                        SpatialDropout2D(0.2),
                        BatchNormalization(),
                        MaxPooling2D(pool_size=2),
                        Flatten(),
                        RegularizedDense(4096),
                        BatchNormalization(),
                        RegularizedDense(2048),
                        Dropout(0.1),
                        RegularizedDense(1024),
                        Dropout(0.2),
                        RegularizedDense(512),
                        Dropout(0.2),
                        Dense(1,activation='sigmoid')
  ])
  cnn_model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])
  return cnn_model

CNN_model=create_model()

CNN_model.summary()

CNN_model.fit(X_train,y_train,batch_size=100,epochs=250,verbose=2,validation_data=(X_val,y_val))

from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from tensorflow.keras.preprocessing import image
import matplotlib.image as mpimg

# first I will apply CLAHE to improve image contrast to provide more prominent features 
cv2_imshow(cv2.resize(X_train[2], (0,0), fx=2.5, fy=2.5))# This is the image before

plt.hist(X_train[2].flat,bins=100,range=(0,255))

#Histogram Equalization
equalized_1=cv2.equalizeHist(X_train[2])
cv2_imshow(cv2.resize(equalized_1, (0,0), fx=2.5, fy=2.5))

plt.hist(equalized_1.flat,bins=100,range=(0,255))

#Contrast Limiting Adaptive Histogram Equalization

clahe=cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))
equalized=clahe.apply(X_train[2])
cv2_imshow(cv2.resize(equalized, (0,0), fx=2.5, fy=2.5))

plt.imshow(equalized,cmap='gray')

plt.hist(equalized.flat,bins=100,range=(0,255))

#So i will apply CLAHE to all the images instead of Histogram Equalization to avoid the noise
#clahe=cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))
#X=clahe.apply(X)
def create_CLAHE():
  new_X=[]
  for item in X:
    new_X.append(clahe.apply(item))
  return new_X

new_X=np.array(create_CLAHE())

type(new_X)

new_X.shape

new_X=new_X.reshape(-1,100,100,1)

new_X.shape

X_train, X_val, y_train, y_val = train_test_split(new_X,y,test_size=0.2, random_state = 42)

# Commented out IPython magic to ensure Python compatibility.
from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array
from matplotlib import pyplot
# %matplotlib inline

X[0].shape

cv2_imshow(X[0])

# Now we will work on expanding the dataset use imagedatagenerator
datagen=ImageDataGenerator(
    rescale=1. /255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2, 
    zoom_range=0.2,
    fill_mode='nearest'
)


datagen.fit(new_X)

# fits the model on batches with real-time data augmentation:
CNN_model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),#steps_per_epoch=100,
          epochs=200,verbose=2,validation_data=(X_val,y_val))

from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlystop = EarlyStopping(monitor='val_loss',patience=10)
# ReduceLROnPlateau: Reduce learning rate when a metric has stopped improving
learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)


callbacks = [earlystop, learning_rate_reduction]



CNN_model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


CNN_model.summary()

training=CNN_model.fit(datagen.flow(X_train, y_train, batch_size=32),epochs=200,verbose=2,
                   #steps_per_epoch=100,
                   validation_data=(X_val,y_val),
                       validation_steps=100,callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
# list all data in training
print(training.history.keys())
# summarize training for accuracy
plt.plot(training.history['accuracy'])
plt.plot(training.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize traning for loss
plt.plot(training.history['loss'])
plt.plot(training.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

CNN_model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
CNN_model.summary()

training=CNN_model.fit(datagen.flow(X_train, y_train, batch_size=32),epochs=200,verbose=2,
                   #steps_per_epoch=100,
                   validation_data=(X_val,y_val),
                       validation_steps=100,callbacks=callbacks)

"""# Trying Transfer Learning using VGG16"""

from keras.applications import VGG16
from keras import layers
from keras.layers import GlobalMaxPooling2D
from keras.models import Model


image_size = 100
input_shape = (image_size, image_size, 3)


epochs = 20
batch_size = 100

pre_trained_model = VGG16(input_shape=input_shape, include_top=False, weights="imagenet")

for layer in pre_trained_model.layers[:15]:
    layer.trainable = False

for layer in pre_trained_model.layers[15:]:
    layer.trainable = True
    
last_layer = pre_trained_model.get_layer('block5_pool')
last_output = last_layer.output
    
# Flatten the output layer to 1 dimension
x = GlobalMaxPooling2D()(last_output)
# Add a fully connected layer with 512 hidden units and ReLU activation
x = Dense(512, activation='relu')(x)
# Add a dropout rate of 0.5
x = Dropout(0.5)(x)
# Add a final sigmoid layer for classification
x = layers.Dense(1, activation='sigmoid')(x)

model2 = Model(pre_trained_model.input, x)

new_X.shape

data=[]                                                                                 # we created an empty array called data
img_size = 100                                                                          # defined img_size to be 100
def create_data():                                                                      # defined a func to create the data
        for item in ['normal','pneumonia']:                                             # for loop that iterates for both items of the list separately!
            path='/content/content/pneumonia_dataset/train/' + item+"/"
            
            for img in os.listdir(path):                                                # os.listdir gets you all the list of name of files located in the given path, " for each image do the following"
                try:
                    img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)   # converts the image to pixels and gray scales the images
                    new_img_array=cv2.resize(img_array,(img_size,img_size))
                    # print(img_array)
                    if item == 'normal':
                        data.append([new_img_array,0])
                    else:
                        data.append([new_img_array, 1])                                 # appending the list of image pixels and respective target value in data
                except Exception as e:
                    pass                                                                # try and except is exception handling case in python, saves you from getting errors
                
            
create_data()

np.random.shuffle(data)

X = []
y = []
for image in data:
  X.append(image[0])
  y.append(image[1])

X=np.array(X)
y=np.array(y)

datagen=ImageDataGenerator(
    rescale=1. /255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2, 
    zoom_range=0.2,
    fill_mode='nearest'
)


datagen.fit(X)

X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2, random_state = 42)

model2.compile(loss='binary_crossentropy',
              optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),
              metrics=['accuracy'])

model2.summary()

len(X_train)

len(X_val)

training=model2.fit(datagen.flow(X_train, y_train, batch_size=20),verbose=2,
                   steps_per_epoch = len(X_train)//batch_size,epochs=200,batch_size=20,
                   validation_data=(X_val,y_val),
                       validation_steps = len(X_val)//batch_size,callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
# list all data in training
print(training.history.keys())
# summarize training for accuracy
plt.plot(training.history['accuracy'])
plt.plot(training.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize traning for loss
plt.plot(training.history['loss'])
plt.plot(training.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

